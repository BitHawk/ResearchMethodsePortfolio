# Reflective Activity 1 – Ethics in Computing in the Age of Generative AI

## Introduction
The accelerated evolution and implementation of generative artificial intelligence (AI) technologies have prompted an imperative discourse on the ethical obligations of computing professionals. Although the concept of AI is not new, recent advancements in generative models have introduced significant ethical, social, and regulatory challenges. These models, capable of producing text, images, and software code, have achieved a level of autonomy and scale previously unattainable.

This reflection explores the ethical foundations and global governance landscape surrounding generative AI, focusing on the principles outlined in the Association for Computing Machinery (ACM) Code of Ethics and Professional Conduct (2018), the comparative review of global AI ethics guidelines by Jobin, Ienca and Vayena (2019), and two key European regulatory instruments: the General Data Protection Regulation (GDPR, 2016) and the forthcoming EU Artificial Intelligence Act (2024). Collectively, these frameworks demonstrate the need for ethical computing to balance innovation with accountability, transparency, and respect for fundamental rights.

## Ethics in Computing and the ACM Code
The ACM Code of Ethics and Professional Conduct (2018) is widely regarded as one of the most comprehensive normative frameworks for computing professionals. It establishes fundamental obligations, including the avoidance of harm, honesty and trustworthiness, fairness and non-discrimination, and respect for privacy and intellectual property. These principles underpin the ethical considerations surrounding generative AI systems.

In the context of AI development, the obligation to avoid harm is particularly significant. The use of generative models has the potential to disseminate misinformation, perpetuate social biases, or produce malicious content such as deepfakes or phishing templates. The concept of harm in computing therefore encompasses not only physical damage but also reputational, psychological, and societal consequences.

Similarly, the principles of honesty and trustworthiness require transparency in how AI systems operate. Yet many generative models remain opaque “black boxes,” hindering users’ understanding of how outputs are produced and what data they rely on. The ACM framework therefore calls upon professionals to design and deploy systems whose limitations are clearly communicated and whose outcomes can be verified.

Another essential dimension is respect for privacy. Generative AI tools frequently depend on large quantities of training data that may contain personal or sensitive information. According to the ACM Code, professionals are obligated to safeguard the confidentiality of such data and to use it only for legitimate purposes. The challenge lies in the fact that large-scale datasets are rarely fully auditable, creating uncertainty about whether consent has been obtained or data has been adequately anonymised.

## A Worldwide Survey of Artificial Intelligence Ethics
Jobin, Ienca and Vayena (2019) conducted one of the first systematic mappings of the global AI ethics landscape, analysing more than 80 guidelines published by governments, companies, and research institutions worldwide. Their findings reveal a broad convergence around several key principles—transparency, justice and fairness, non-maleficence, responsibility, and privacy—but also a striking lack of mechanisms for enforcement and accountability. The authors describe this as an “ethics gap”: the proliferation of aspirational principles without corresponding operationalisation or oversight.

This comparative perspective exposes the interplay between cultural diversity and governance fragmentation. European guidelines tend to emphasise human rights and social welfare, while North American frameworks prioritise innovation and voluntary responsibility. East Asian policies, in contrast, often stress harmony and collective benefit. The absence of global consensus creates challenges for transnational technology actors that must navigate overlapping or conflicting ethical expectations.

The study further emphasises that ethical principles alone are inadequate without institutional structures to translate them into practice. The authors advocate stronger connections between ethics and law, arguing that normative consensus must be embedded in regulatory design to ensure accountability. This insight is increasingly reflected in European initiatives such as the GDPR and the forthcoming AI Act.

## European Regulatory Context
The General Data Protection Regulation (GDPR, 2016) is widely regarded as a milestone in global data ethics. It establishes principles of lawfulness, fairness, transparency, and data minimisation, while granting individuals the right to access, correct, and erase their data, as well as protection against automated decision-making that significantly affects them. These provisions constrain how personal data can be collected and processed for model training and inference in AI systems. The regulation is founded on the concept of data protection by design and by default, transforming ethical imperatives into legal obligations.

Building on this foundation, the EU Artificial Intelligence Act (2024) introduces a risk-based framework for regulating AI according to its potential impact on safety, fundamental rights, and trust. The Act categorises AI systems into three groups—prohibited, high-risk, and low-risk—and imposes proportionate obligations such as transparency documentation, human oversight, and post-market monitoring. Generative AI systems are subject to transparency requirements mandating clear disclosure when users interact with AI-generated content and when datasets include copyrighted or personal material.

The combined impact of the GDPR and the AI Act reflects a European approach that treats ethics not as voluntary guidance but as enforceable accountability. This model contrasts with self-regulatory approaches elsewhere and exemplifies an effort to institutionalise ethical computing through legislative instruments. However, the EU model also presents implementation challenges, including compliance costs, the pace of innovation, and the difficulty of verifying adherence within complex global AI supply chains.

## Challenges in Operationalising Ethics
Despite broad consensus on high-level principles, translating ethics into technical and organisational practice remains difficult. Many ethical frameworks rely on ambiguous concepts such as fairness or transparency, which lack universally accepted metrics. Developers and auditors often disagree on how to measure algorithmic bias or explainability.

The multidisciplinary nature of AI ethics—spanning computer science, law, sociology, and philosophy—means that no single profession can claim ownership of the issue. Effective governance therefore depends on interdisciplinary collaboration and continuous review.

A related concern is the responsibility gap. Determining accountability when AI systems generate harmful or misleading content is complex: is the developer responsible for design flaws, the data provider for biases, or the user for misuse? The ACM Code stipulates that computing professionals are accountable for both the direct and indirect consequences of their work. Yet real-world enforcement remains limited. Mechanisms such as ethical review boards, model cards, and algorithmic impact assessments are emerging, but adoption remains uneven and often symbolic.

Another challenge is the tension between innovation and regulation. Critics argue that excessive compliance requirements may impede research and competitiveness, whereas proponents contend that responsible innovation is essential for sustainable trust. The EU AI Act attempts to balance these concerns through proportionality—imposing stricter obligations on high-risk systems while promoting innovation in low-risk applications. Achieving this balance will determine whether ethics becomes an enabler or an obstacle to technological progress.

## Towards an Ethically Sustainable Future
The reviewed sources collectively suggest an evolving paradigm in which ethics in computing is no longer discretionary or peripheral but integral to system design and governance. The ACM Code provides the moral foundation, the analysis by Jobin et al. (2019) exposes the global diversity and fragmentation of ethical norms, and the GDPR together with the AI Act demonstrate how Europe seeks to embed ethical values within binding regulation.

The future of ethical AI will depend on the successful implementation of three complementary strategies:

1. Embedding ethics in design: Ethical reflection must accompany technical decision-making throughout the AI lifecycle. Techniques such as ethics-by-design and explainable AI can make moral principles explicit within algorithms and interfaces.  
2. Institutionalising accountability: Organisations developing or deploying AI should establish clear roles for oversight, risk assessment, and redress. Independent auditing and transparency reports can strengthen public trust.  
3. Promoting international dialogue: Because AI systems operate across borders, global collaboration is necessary to harmonise standards and prevent ethical arbitrage. Initiatives such as the OECD and UNESCO guidelines can serve as platforms for convergence.  

The fundamental ethical challenge posed by generative AI is not to halt innovation but to ensure that progress aligns with human dignity and societal welfare. The ACM and European regulators conceptualise ethical computing as a practice requiring professionals to integrate technical expertise with moral reasoning and legal awareness. As AI continues to evolve, maintaining this balance will be essential to sustain trust in both technology and the profession that creates it.

## References
Association for Computing Machinery (ACM). (2018) ACM Code of Ethics and Professional Conduct. Available at: https://www.acm.org/code-of-ethics

European Parliament and Council. (2016) General Data Protection Regulation (EU 2016/679).

European Union. (2024) Artificial Intelligence Act. Available at: https://artificial-intelligence-act.eu/

Jobin, A., Ienca, M. and Vayena, E. (2019) ‘The global landscape of AI ethics guidelines’, Nature Machine Intelligence, 1(9), pp. 389–399. https://doi.org/10.1038/s42256-019-0088-2
